{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10afd79c",
   "metadata": {},
   "source": [
    "# Training a model for correct first page prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d9053",
   "metadata": {},
   "source": [
    "This notebook covers one of the approaches to training a model for predicting whether a page of the document is the first one or not -- a feature that would allow correct splitting for PDFs that consist of more than one actual document (we assume that the pages are already sorted). The approach used is NBOW (Neural Bag-of-words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b2125",
   "metadata": {},
   "source": [
    "Before you start, makee sure you have **installed** and **initialized** the konfuzio_sdk package as shown in the readme of the [repository](https://github.com/konfuzio-ai/Python-SDK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konfuzio-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "!konfuzio_sdk init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80734a86",
   "metadata": {},
   "source": [
    "Importing necessary libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53858247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from konfuzio_sdk.data import Project, Document\n",
    "from nltk import word_tokenize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a707c",
   "metadata": {},
   "source": [
    "Setting seed for reproducibility purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72d86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636852e",
   "metadata": {},
   "source": [
    "We will use a multilayered perceptron architecture built with Keras library and a vocabulary built by using Counter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d6d22",
   "metadata": {},
   "source": [
    "### Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579d2b6",
   "metadata": {},
   "source": [
    "Loading our project for training and testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1de0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_project = Project(id_=1644)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02180ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_project.documents\n",
    "test_data = my_project.test_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f195fd",
   "metadata": {},
   "source": [
    "Preparing data for training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1488016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1443/1443 [00:02<00:00, 712.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_texts = []\n",
    "train_data_labels = []\n",
    "\n",
    "for doc in tqdm(train_data):\n",
    "    for page in doc.pages():\n",
    "        train_data_texts.append(page.text)\n",
    "        if page.number == 1:\n",
    "            train_data_labels.append(1)\n",
    "        elif page.number != 1 and int(page.number):\n",
    "            train_data_labels.append(0)\n",
    "        else:\n",
    "            print(page.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ebf15a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 286/286 [00:00<00:00, 644.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_texts = []\n",
    "test_data_labels = []\n",
    "\n",
    "for doc in tqdm(test_data):\n",
    "    for page in doc.pages():\n",
    "        test_data_texts.append(page.text)\n",
    "        if page.number == 1:\n",
    "            test_data_labels.append(1)\n",
    "        elif page.number != 1 and int(page.number):\n",
    "            test_data_labels.append(0)\n",
    "        else:\n",
    "            print(page.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384388f",
   "metadata": {},
   "source": [
    "### NBOW (no preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03364ea3",
   "metadata": {},
   "source": [
    "Initializing and building the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c24fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32592b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2634/2634 [00:07<00:00, 365.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(train_data_texts):\n",
    "    tokens = word_tokenize(text)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c77e4b",
   "metadata": {},
   "source": [
    "Intializing and fitting the tokenizer for subsequent applying at the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a0d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97812f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_data_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "328089a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2634, 49404)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = tokenizer.texts_to_matrix(train_data_texts, mode='freq')\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df45169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435, 49404)\n"
     ]
    }
   ],
   "source": [
    "Xtest = tokenizer.texts_to_matrix(test_data_texts, mode='freq')\n",
    "print(Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f3651",
   "metadata": {},
   "source": [
    "Processing the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d420ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = np.array(train_data_labels)\n",
    "ytest = np.array(test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = Xtest.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bbc0",
   "metadata": {},
   "source": [
    "The architecture is Keras's Sequential with two Dense layers. The training runs for 50 epochs; chosen metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b9c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 14:24:15.461986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "83/83 - 2s - loss: 0.5283 - accuracy: 0.7388 - 2s/epoch - 23ms/step\n",
      "Epoch 2/100\n",
      "83/83 - 1s - loss: 0.2112 - accuracy: 0.9127 - 898ms/epoch - 11ms/step\n",
      "Epoch 3/100\n",
      "83/83 - 1s - loss: 0.0762 - accuracy: 0.9749 - 1s/epoch - 13ms/step\n",
      "Epoch 4/100\n",
      "83/83 - 1s - loss: 0.0335 - accuracy: 0.9913 - 917ms/epoch - 11ms/step\n",
      "Epoch 5/100\n",
      "83/83 - 1s - loss: 0.0249 - accuracy: 0.9916 - 914ms/epoch - 11ms/step\n",
      "Epoch 6/100\n",
      "83/83 - 1s - loss: 0.0204 - accuracy: 0.9939 - 924ms/epoch - 11ms/step\n",
      "Epoch 7/100\n",
      "83/83 - 1s - loss: 0.0156 - accuracy: 0.9943 - 1s/epoch - 14ms/step\n",
      "Epoch 8/100\n",
      "83/83 - 1s - loss: 0.0157 - accuracy: 0.9932 - 996ms/epoch - 12ms/step\n",
      "Epoch 9/100\n",
      "83/83 - 1s - loss: 0.0131 - accuracy: 0.9954 - 935ms/epoch - 11ms/step\n",
      "Epoch 10/100\n",
      "83/83 - 1s - loss: 0.0138 - accuracy: 0.9947 - 1s/epoch - 12ms/step\n",
      "Epoch 11/100\n",
      "83/83 - 1s - loss: 0.0121 - accuracy: 0.9954 - 1s/epoch - 13ms/step\n",
      "Epoch 12/100\n",
      "83/83 - 1s - loss: 0.0105 - accuracy: 0.9951 - 963ms/epoch - 12ms/step\n",
      "Epoch 13/100\n",
      "83/83 - 1s - loss: 0.0088 - accuracy: 0.9966 - 1s/epoch - 14ms/step\n",
      "Epoch 14/100\n",
      "83/83 - 1s - loss: 0.0096 - accuracy: 0.9954 - 999ms/epoch - 12ms/step\n",
      "Epoch 15/100\n",
      "83/83 - 1s - loss: 0.0107 - accuracy: 0.9962 - 1s/epoch - 13ms/step\n",
      "Epoch 16/100\n",
      "83/83 - 1s - loss: 0.0107 - accuracy: 0.9954 - 998ms/epoch - 12ms/step\n",
      "Epoch 17/100\n",
      "83/83 - 1s - loss: 0.0127 - accuracy: 0.9962 - 967ms/epoch - 12ms/step\n",
      "Epoch 18/100\n",
      "83/83 - 1s - loss: 0.0083 - accuracy: 0.9970 - 955ms/epoch - 12ms/step\n",
      "Epoch 19/100\n",
      "83/83 - 1s - loss: 0.0120 - accuracy: 0.9958 - 962ms/epoch - 12ms/step\n",
      "Epoch 20/100\n",
      "83/83 - 1s - loss: 0.0132 - accuracy: 0.9954 - 938ms/epoch - 11ms/step\n",
      "Epoch 21/100\n",
      "83/83 - 1s - loss: 0.0073 - accuracy: 0.9970 - 1s/epoch - 13ms/step\n",
      "Epoch 22/100\n",
      "83/83 - 1s - loss: 0.0094 - accuracy: 0.9954 - 1s/epoch - 12ms/step\n",
      "Epoch 23/100\n",
      "83/83 - 1s - loss: 0.0159 - accuracy: 0.9954 - 1s/epoch - 13ms/step\n",
      "Epoch 24/100\n",
      "83/83 - 1s - loss: 0.0071 - accuracy: 0.9962 - 1s/epoch - 14ms/step\n",
      "Epoch 25/100\n",
      "83/83 - 1s - loss: 0.0089 - accuracy: 0.9973 - 1s/epoch - 18ms/step\n",
      "Epoch 26/100\n",
      "83/83 - 1s - loss: 0.0088 - accuracy: 0.9970 - 1s/epoch - 17ms/step\n",
      "Epoch 27/100\n",
      "83/83 - 1s - loss: 0.0106 - accuracy: 0.9958 - 1s/epoch - 16ms/step\n",
      "Epoch 28/100\n",
      "83/83 - 1s - loss: 0.0089 - accuracy: 0.9970 - 1s/epoch - 13ms/step\n",
      "Epoch 29/100\n",
      "83/83 - 1s - loss: 0.0118 - accuracy: 0.9954 - 989ms/epoch - 12ms/step\n",
      "Epoch 30/100\n",
      "83/83 - 1s - loss: 0.0090 - accuracy: 0.9966 - 976ms/epoch - 12ms/step\n",
      "Epoch 31/100\n",
      "83/83 - 1s - loss: 0.0085 - accuracy: 0.9958 - 1s/epoch - 12ms/step\n",
      "Epoch 32/100\n",
      "83/83 - 1s - loss: 0.0089 - accuracy: 0.9951 - 968ms/epoch - 12ms/step\n",
      "Epoch 33/100\n",
      "83/83 - 1s - loss: 0.0086 - accuracy: 0.9958 - 976ms/epoch - 12ms/step\n",
      "Epoch 34/100\n",
      "83/83 - 1s - loss: 0.0084 - accuracy: 0.9958 - 955ms/epoch - 12ms/step\n",
      "Epoch 35/100\n",
      "83/83 - 1s - loss: 0.0075 - accuracy: 0.9958 - 987ms/epoch - 12ms/step\n",
      "Epoch 36/100\n",
      "83/83 - 1s - loss: 0.0079 - accuracy: 0.9973 - 993ms/epoch - 12ms/step\n",
      "Epoch 37/100\n",
      "83/83 - 1s - loss: 0.0094 - accuracy: 0.9966 - 1s/epoch - 13ms/step\n",
      "Epoch 38/100\n",
      "83/83 - 1s - loss: 0.0088 - accuracy: 0.9958 - 1s/epoch - 12ms/step\n",
      "Epoch 39/100\n",
      "83/83 - 1s - loss: 0.0060 - accuracy: 0.9966 - 1s/epoch - 15ms/step\n",
      "Epoch 40/100\n",
      "83/83 - 2s - loss: 0.0068 - accuracy: 0.9970 - 2s/epoch - 19ms/step\n",
      "Epoch 41/100\n",
      "83/83 - 2s - loss: 0.0065 - accuracy: 0.9981 - 2s/epoch - 20ms/step\n",
      "Epoch 42/100\n",
      "83/83 - 1s - loss: 0.0080 - accuracy: 0.9970 - 1s/epoch - 18ms/step\n",
      "Epoch 43/100\n",
      "83/83 - 1s - loss: 0.0111 - accuracy: 0.9962 - 1s/epoch - 14ms/step\n",
      "Epoch 44/100\n",
      "83/83 - 1s - loss: 0.0060 - accuracy: 0.9977 - 1s/epoch - 15ms/step\n",
      "Epoch 45/100\n",
      "83/83 - 1s - loss: 0.0100 - accuracy: 0.9954 - 1s/epoch - 12ms/step\n",
      "Epoch 46/100\n",
      "83/83 - 1s - loss: 0.0074 - accuracy: 0.9977 - 1s/epoch - 12ms/step\n",
      "Epoch 47/100\n",
      "83/83 - 1s - loss: 0.0072 - accuracy: 0.9970 - 1s/epoch - 13ms/step\n",
      "Epoch 48/100\n",
      "83/83 - 1s - loss: 0.0076 - accuracy: 0.9966 - 1s/epoch - 13ms/step\n",
      "Epoch 49/100\n",
      "83/83 - 1s - loss: 0.0055 - accuracy: 0.9981 - 1s/epoch - 13ms/step\n",
      "Epoch 50/100\n",
      "83/83 - 1s - loss: 0.0066 - accuracy: 0.9973 - 1s/epoch - 12ms/step\n",
      "Epoch 51/100\n",
      "83/83 - 1s - loss: 0.0069 - accuracy: 0.9962 - 1s/epoch - 12ms/step\n",
      "Epoch 52/100\n",
      "83/83 - 1s - loss: 0.0073 - accuracy: 0.9970 - 1s/epoch - 13ms/step\n",
      "Epoch 53/100\n",
      "83/83 - 1s - loss: 0.0070 - accuracy: 0.9958 - 1s/epoch - 12ms/step\n",
      "Epoch 54/100\n",
      "83/83 - 1s - loss: 0.0101 - accuracy: 0.9962 - 1s/epoch - 12ms/step\n",
      "Epoch 55/100\n",
      "83/83 - 1s - loss: 0.0048 - accuracy: 0.9977 - 1s/epoch - 12ms/step\n",
      "Epoch 56/100\n",
      "83/83 - 1s - loss: 0.0066 - accuracy: 0.9966 - 1s/epoch - 12ms/step\n",
      "Epoch 57/100\n",
      "83/83 - 1s - loss: 0.0059 - accuracy: 0.9962 - 1s/epoch - 12ms/step\n",
      "Epoch 58/100\n",
      "83/83 - 1s - loss: 0.0072 - accuracy: 0.9966 - 1s/epoch - 12ms/step\n",
      "Epoch 59/100\n",
      "83/83 - 1s - loss: 0.0058 - accuracy: 0.9973 - 1s/epoch - 16ms/step\n",
      "Epoch 60/100\n",
      "83/83 - 1s - loss: 0.0079 - accuracy: 0.9970 - 1s/epoch - 12ms/step\n",
      "Epoch 61/100\n",
      "83/83 - 1s - loss: 0.0063 - accuracy: 0.9958 - 1s/epoch - 12ms/step\n",
      "Epoch 62/100\n",
      "83/83 - 1s - loss: 0.0051 - accuracy: 0.9973 - 1s/epoch - 13ms/step\n",
      "Epoch 63/100\n",
      "83/83 - 1s - loss: 0.0052 - accuracy: 0.9977 - 1s/epoch - 13ms/step\n",
      "Epoch 64/100\n",
      "83/83 - 1s - loss: 0.0072 - accuracy: 0.9966 - 1s/epoch - 12ms/step\n",
      "Epoch 65/100\n",
      "83/83 - 1s - loss: 0.0069 - accuracy: 0.9977 - 1s/epoch - 12ms/step\n",
      "Epoch 66/100\n",
      "83/83 - 1s - loss: 0.0066 - accuracy: 0.9966 - 1s/epoch - 13ms/step\n",
      "Epoch 67/100\n",
      "83/83 - 1s - loss: 0.0069 - accuracy: 0.9962 - 1s/epoch - 15ms/step\n",
      "Epoch 68/100\n",
      "83/83 - 2s - loss: 0.0062 - accuracy: 0.9973 - 2s/epoch - 19ms/step\n",
      "Epoch 69/100\n",
      "83/83 - 1s - loss: 0.0073 - accuracy: 0.9958 - 1s/epoch - 16ms/step\n",
      "Epoch 70/100\n",
      "83/83 - 1s - loss: 0.0073 - accuracy: 0.9962 - 1s/epoch - 18ms/step\n",
      "Epoch 71/100\n",
      "83/83 - 1s - loss: 0.0055 - accuracy: 0.9970 - 1s/epoch - 16ms/step\n",
      "Epoch 72/100\n",
      "83/83 - 1s - loss: 0.0062 - accuracy: 0.9962 - 1s/epoch - 13ms/step\n",
      "Epoch 73/100\n",
      "83/83 - 1s - loss: 0.0067 - accuracy: 0.9966 - 1s/epoch - 15ms/step\n",
      "Epoch 74/100\n",
      "83/83 - 2s - loss: 0.0050 - accuracy: 0.9970 - 2s/epoch - 19ms/step\n",
      "Epoch 75/100\n",
      "83/83 - 1s - loss: 0.0064 - accuracy: 0.9970 - 1s/epoch - 14ms/step\n",
      "Epoch 76/100\n",
      "83/83 - 1s - loss: 0.0127 - accuracy: 0.9951 - 1s/epoch - 16ms/step\n",
      "Epoch 77/100\n",
      "83/83 - 2s - loss: 0.0073 - accuracy: 0.9977 - 2s/epoch - 21ms/step\n",
      "Epoch 78/100\n",
      "83/83 - 2s - loss: 0.0064 - accuracy: 0.9970 - 2s/epoch - 27ms/step\n",
      "Epoch 79/100\n",
      "83/83 - 2s - loss: 0.0057 - accuracy: 0.9966 - 2s/epoch - 21ms/step\n",
      "Epoch 80/100\n",
      "83/83 - 2s - loss: 0.0054 - accuracy: 0.9966 - 2s/epoch - 20ms/step\n",
      "Epoch 81/100\n",
      "83/83 - 2s - loss: 0.0061 - accuracy: 0.9973 - 2s/epoch - 23ms/step\n",
      "Epoch 82/100\n",
      "83/83 - 1s - loss: 0.0062 - accuracy: 0.9977 - 1s/epoch - 16ms/step\n",
      "Epoch 83/100\n",
      "83/83 - 1s - loss: 0.0053 - accuracy: 0.9973 - 1s/epoch - 18ms/step\n",
      "Epoch 84/100\n",
      "83/83 - 2s - loss: 0.0051 - accuracy: 0.9970 - 2s/epoch - 19ms/step\n",
      "Epoch 85/100\n",
      "83/83 - 2s - loss: 0.0044 - accuracy: 0.9981 - 2s/epoch - 20ms/step\n",
      "Epoch 86/100\n",
      "83/83 - 2s - loss: 0.0076 - accuracy: 0.9962 - 2s/epoch - 21ms/step\n",
      "Epoch 87/100\n",
      "83/83 - 2s - loss: 0.0058 - accuracy: 0.9962 - 2s/epoch - 21ms/step\n",
      "Epoch 88/100\n",
      "83/83 - 2s - loss: 0.0049 - accuracy: 0.9973 - 2s/epoch - 23ms/step\n",
      "Epoch 89/100\n",
      "83/83 - 2s - loss: 0.0055 - accuracy: 0.9973 - 2s/epoch - 25ms/step\n",
      "Epoch 90/100\n",
      "83/83 - 2s - loss: 0.0049 - accuracy: 0.9970 - 2s/epoch - 24ms/step\n",
      "Epoch 91/100\n",
      "83/83 - 2s - loss: 0.0055 - accuracy: 0.9977 - 2s/epoch - 24ms/step\n",
      "Epoch 92/100\n",
      "83/83 - 2s - loss: 0.0066 - accuracy: 0.9966 - 2s/epoch - 22ms/step\n",
      "Epoch 93/100\n",
      "83/83 - 2s - loss: 0.0053 - accuracy: 0.9970 - 2s/epoch - 19ms/step\n",
      "Epoch 94/100\n",
      "83/83 - 2s - loss: 0.0049 - accuracy: 0.9970 - 2s/epoch - 24ms/step\n",
      "Epoch 95/100\n",
      "83/83 - 1s - loss: 0.0054 - accuracy: 0.9973 - 1s/epoch - 16ms/step\n",
      "Epoch 96/100\n",
      "83/83 - 1s - loss: 0.0062 - accuracy: 0.9981 - 1s/epoch - 16ms/step\n",
      "Epoch 97/100\n",
      "83/83 - 2s - loss: 0.0055 - accuracy: 0.9973 - 2s/epoch - 20ms/step\n",
      "Epoch 98/100\n",
      "83/83 - 1s - loss: 0.0051 - accuracy: 0.9966 - 1s/epoch - 16ms/step\n",
      "Epoch 99/100\n",
      "83/83 - 2s - loss: 0.0061 - accuracy: 0.9962 - 2s/epoch - 18ms/step\n",
      "Epoch 100/100\n",
      "83/83 - 1s - loss: 0.0054 - accuracy: 0.9962 - 1s/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feeeb2a9c40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "model.add(Dense(50, activation='elu'))\n",
    "model.add(Dense(50, activation='elu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, ytrain, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e5b97",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba012b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NBOW.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795753a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 10:22:10.999199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('NBOW.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b102495",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47241b2c",
   "metadata": {},
   "source": [
    "### Metrics & prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0b972",
   "metadata": {},
   "source": [
    "Accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3da44cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.85057330131531\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc624dc8",
   "metadata": {},
   "source": [
    "A function for running predictions manually consists of pre-filtering with the usage of previously built vocabulary and the prediction on the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(page_text, vocab, tokenizer, model):\n",
    "    tokens = word_tokenize(page_text)\n",
    "    tokens = [t for t in tokens if t in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    pred = model.predict(encoded, verbose=0)\n",
    "    return round(pred[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d570e",
   "metadata": {},
   "source": [
    "We calculate our custom metric via the following function that determines how many ground-truth first pages were actually predicted as first pages. The logic behind this approach suggests that by determining first pages correctly we can consecutively split documents correctly, using each first page as a separator (since it means a start of a new document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83846d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(texts, labels):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for i, test in tqdm(zip(labels, texts)):\n",
    "        pred = predict_label(test, vocab, tokenizer, model)\n",
    "        if i == 1 and pred == 1:\n",
    "            true_positive += 1\n",
    "        elif i == 1 and pred == 0:\n",
    "            false_negative += 1\n",
    "        elif i == 0 and pred == 1:\n",
    "            false_positive += 1\n",
    "    \n",
    "    if true_positive + false_positive != 0:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    if true_positive + false_negative != 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "    \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        f1 = 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = calculate_metrics(test_data_texts, test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fae37a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision: 0.7642276422764228 \n",
      " Recall: 0.986013986013986 \n",
      " F1-score: 0.8610687022900764\n"
     ]
    }
   ],
   "source": [
    "print('\\n Precision: {} \\n Recall: {} \\n F1-score: {}'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414e8c2",
   "metadata": {},
   "source": [
    "Manual assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef74a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for test in test_data_texts[:10]: \n",
    "    print(predict_label(test, vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3acd7490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cc242",
   "metadata": {},
   "source": [
    "The results for the manual assessment prove to be similar to the evaluation given previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a05a65",
   "metadata": {},
   "source": [
    "Let us make some visualizations to ensure the manually-run predictions are correct as well.\n",
    "First, let's take a look at a single-page document which is the first in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f19c0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document 32.pdf (334665)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373604a",
   "metadata": {},
   "source": [
    "Since it's a single-page document, it only has the first page, and it was predicted as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db0bc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 1 , prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[0] , \", prediction:\", predict_label(test_data_texts[0], vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920220e7",
   "metadata": {},
   "source": [
    "Next, we'll take a look at a two-page document which is also present in the test set. Its first page should be predicted to be the first (receive label 1) , and the second one should be predicted as not first (receive label 0), and it has been predicted as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c6cc9a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document hit_20200711_002.pdf (334821)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beacb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 1 , prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[7] , \", prediction:\", predict_label(test_data_texts[7], vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b7a3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 0 , prediction: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[8] , \", prediction:\", predict_label(test_data_texts[8], vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab95f92",
   "metadata": {},
   "source": [
    "Not all the pages get predicted correctly. Let's take a look at the three-page document that got 2 non-first pages predicted as first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b92b8d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 1 , prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[28] , \", prediction:\", predict_label(test_data_texts[28], vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "974923e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = my_project.get_document_by_id(334946).pages()[0].image_path\n",
    "path_2 = my_project.get_document_by_id(334946).pages()[1].image_path\n",
    "path_3 = my_project.get_document_by_id(334946).pages()[2].image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8456d",
   "metadata": {},
   "source": [
    "The following page gets predicted incorrectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afbf6c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 0 , prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[29] , \", prediction:\", predict_label(test_data_texts[29], vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38418c70",
   "metadata": {},
   "source": [
    "The third page also gets an incorrect prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d8d7575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original label: 0 , prediction: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"original label:\", test_data_labels[30] , \", prediction:\", predict_label(test_data_texts[30], vocab, tokenizer, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
