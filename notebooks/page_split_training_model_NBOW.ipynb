{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10afd79c",
   "metadata": {},
   "source": [
    "# Training a model for correct first page prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d9053",
   "metadata": {},
   "source": [
    "This notebook covers one of the approaches to training a model for predicting whether a page of the document is the first one or not -- a feature that would allow correct splitting for PDFs that consist of more than one actual document (we assume that the pages are already sorted). The approach used is NBOW (Neural Bag-of-words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b2125",
   "metadata": {},
   "source": [
    "Before you start, makee sure you have **installed** and **initialized** the konfuzio_sdk package as shown in the readme of the [repository](https://github.com/konfuzio-ai/Python-SDK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konfuzio-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "!konfuzio_sdk init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80734a86",
   "metadata": {},
   "source": [
    "Importing necessary libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53858247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from konfuzio_sdk.data import Project, Document\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a707c",
   "metadata": {},
   "source": [
    "Setting seed for reproducibility purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72d86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 77\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636852e",
   "metadata": {},
   "source": [
    "We will use a multilayered perceptron architecture built with Keras library and a vocabulary built by using Counter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d6d22",
   "metadata": {},
   "source": [
    "### Gathering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579d2b6",
   "metadata": {},
   "source": [
    "Loading our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1de0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_project = Project(id_=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02180ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = my_project.documents\n",
    "test_docs = my_project.test_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f195fd",
   "metadata": {},
   "source": [
    "Preparing data for training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ec8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_train_docs = []\n",
    "pages_labels = []\n",
    "\n",
    "for doc in tqdm(train_docs):\n",
    "    for page in doc.pages():\n",
    "        pages_train_docs.append(page.text)\n",
    "        if page.number == 1:\n",
    "            pages_labels.append(1)\n",
    "        else:\n",
    "            pages_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_test_docs = []\n",
    "pages_labels_test = []\n",
    "\n",
    "for doc in tqdm(test_docs):\n",
    "    for page in doc.pages():\n",
    "        pages_test_docs.append(page.text)\n",
    "        if page.number == 1:\n",
    "            pages_labels_test.append(1)\n",
    "        else:\n",
    "            pages_labels_test.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384388f",
   "metadata": {},
   "source": [
    "### NBOW (no preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03364ea3",
   "metadata": {},
   "source": [
    "Initializing and building the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32592b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in tqdm(pages_train_docs):\n",
    "    tokens = word_tokenize(text)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c77e4b",
   "metadata": {},
   "source": [
    "Intializing and fitting the tokenizer for subsequent applying at the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a0d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97812f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(pages_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328089a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tokenizer.texts_to_matrix(pages_train_docs, mode='freq')\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df45169",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = tokenizer.texts_to_matrix(pages_test_docs, mode='freq')\n",
    "print(Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f3651",
   "metadata": {},
   "source": [
    "Processing the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d420ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = np.array(pages_labels)\n",
    "ytest = np.array(pages_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = Xtest.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bbc0",
   "metadata": {},
   "source": [
    "The architecture is Keras's Sequential with two Dense layers. The training runs for 50 epochs; chosen metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, ytrain, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dfbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47241b2c",
   "metadata": {},
   "source": [
    "### Metrics & prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0b972",
   "metadata": {},
   "source": [
    "Accuracy on the test set is 95%, which is unrealistically high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "153982d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.31915187835693\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc624dc8",
   "metadata": {},
   "source": [
    "A function for running predictions manually consists of pre-filtering with the usage of previously built vocabulary and the prediction on the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(page_text, vocab, tokenizer, model):\n",
    "    tokens = word_tokenize(page_text)\n",
    "    tokens = [t for t in tokens if t in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    pred = model.predict(encoded, verbose=0)\n",
    "    return round(pred[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83846d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(texts, labels):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for i, test in tqdm(zip(labels, texts)):\n",
    "        pred = predict_label(test, vocab, tokenizer, model)\n",
    "        if i == 1 and pred == 1:\n",
    "            true_positive += 1\n",
    "        elif i == 1 and pred == 0:\n",
    "            false_negative += 1\n",
    "        elif i == 0 and pred == 1:\n",
    "            false_positive += 1\n",
    "    \n",
    "    if true_positive + false_positive != 0:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    if true_positive + false_negative != 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "    \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        f1 = 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dadfd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = calculate_metrics(pages_test_docs, pages_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7729c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision: 0.9193548387096774 \n",
      " Recall: 0.9047619047619048 \n",
      " F1-score: 0.912\n"
     ]
    }
   ],
   "source": [
    "print('\\n Precision: {} \\n Recall: {} \\n F1-score: {}'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414e8c2",
   "metadata": {},
   "source": [
    "Manual assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ed6842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for test in pages_test_docs[:10]: \n",
    "    print(predict_label(test, vocab, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a787d23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_labels_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cc242",
   "metadata": {},
   "source": [
    "The results for the manual assessment prove to be similar to the evaluation given previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039860e6",
   "metadata": {},
   "source": [
    "### Running on different data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed99719",
   "metadata": {},
   "source": [
    "Let's check the model's performance on a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ddcbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_project = Project(id_=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3de53d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = my_project.test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_test_docs = []\n",
    "pages_labels_test = []\n",
    "\n",
    "for doc in tqdm(all_docs):\n",
    "    for page in doc.pages():\n",
    "        pages_test_docs.append(page.text)\n",
    "        if page.number == 1:\n",
    "            pages_labels_test.append(1)\n",
    "        else:\n",
    "            pages_labels_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b944917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = tokenizer.texts_to_matrix(pages_test_docs, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62133cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.array(pages_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aac570ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85e29057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.85714328289032\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c4a57",
   "metadata": {},
   "source": [
    "This is significantly different from the initial results we have got on the test set. This might be explained by the fact that the current dataset contains cases more complex than the ones in the initial training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44cc90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 24.46it/s]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = calculate_metrics(pages_test_docs, pages_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9493c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision: 0 \n",
      " Recall: 0.0 \n",
      " F1-score: 0\n"
     ]
    }
   ],
   "source": [
    "print('\\n Precision: {} \\n Recall: {} \\n F1-score: {}'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00c31e",
   "metadata": {},
   "source": [
    "Let's save a new test set into a .csv for further usage with Transformer-based model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4ba98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.DataFrame({'text': pages_test_docs, 'label': pages_labels_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a8ea0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.to_csv('test_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
