<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tokenization &mdash; Konfuzio  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/full_green_square.png"/>
    <link rel="canonical" href="https://dev.konfuzio.com/sdk/tutorials/tokenizers/index.html" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Document Information Extraction" href="../information_extraction/index.html" />
    <link rel="prev" title="File Splitting" href="../file_splitting/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> Konfuzio
            <img src="../../../_static/docs__static_square_transparent_super_small.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Konfuzio SDK</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">What is the Konfuzio SDK?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Get Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../data-preparation/index.html">Prepare Training and Testing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../document_categorization/index.html">Document Categorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../file_splitting/index.html">File Splitting</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tokenization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#whitespacetokenizer">WhitespaceTokenizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-usage-getting-word-bounding-box-bbox-for-a-document">Example Usage: Getting Word Bounding Box (BBox) for a Document</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training-a-label-specific-regex-tokenizer">Training a Label-Specific Regex Tokenizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-usage">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#paragraph-tokenizer">Paragraph Tokenizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#parameters">Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#line-distance-approach">Line Distance Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computer-vision-approach">Computer Vision Approach</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sentence-tokenizer">Sentence Tokenizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Example Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-choose-which-tokenizer-to-use">How to Choose Which Tokenizer to Use?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#verify-that-a-tokenizer-finds-all-labels">Verify that a Tokenizer finds all Labels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../information_extraction/index.html">Document Information Extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../upload-your-ai/index.html">Upload your AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_validation/index.html">Data Validation Rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outlier-annotations/index.html">Find possible outliers among the ground-truth Annotations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../regex_based_annotations/index.html">Create Regex-based Annotations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../async_upload_with_callback/index.html">Pull Documents Uploaded Asynchronously with a Webhook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pdf-form-generator/index.html">Build your own PDF Form Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ner-ontonotes-fast/index.html">Retrain Flair NER-Ontonotes-Fast with Human Revised Annotations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../annual-reports/index.html">Count Relevant Expressions in Annual Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../barcode-scanner/index.html">Barcode Scanner with zxing-cpp</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../explanations.html">Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sourcecode.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribution.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/konfuzio-ai/konfuzio-sdk/releases">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Konfuzio Server</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../web/index.html">What is the Konfuzio Server?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../web/explanations.html">Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../web/api-v3.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../web/on_premises.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../web/passwords/index.html">Password Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../web/changelog_app.html">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Konfuzio Document Validation UI</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../dvui/index.html">What is the Konfuzio Document Validation UI?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dvui/explanations.html">Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dvui/sourcecode.html">Source Code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/konfuzio-ai/document-validation-ui/releases">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Konfuzio</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../tutorials.html">Tutorials</a> &raquo;</li>
      <li>Tokenization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/sdk/tutorials/tokenizers/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>.. _tokenization-tutorials:</p>
<section id="tokenization">
<h1>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h1>
<section id="whitespacetokenizer">
<h2>WhitespaceTokenizer<a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline">¶</a></h2>
<p>The <span class="xref myst">WhitespaceTokenizer</span>,
part of the Konfuzio SDK, is a foundational tool in natural language processing (NLP). It segments text into smaller
units called tokens, using the white spaces between words as natural delimiters. This approach is simple, effective, and
ideal for basic tokenization tasks.</p>
<p>This Tokenizer functions through a regular expression (regex) that identifies sequences of characters unbroken by white
space. For instance, the text “street Name 1-2b,” would be segmented into the tokens “street”, “Name”, and “1-2b,”.</p>
<p>Despite its simplicity, the <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> is a robust tool for many NLP scenarios. Its uncomplicated design makes
it an excellent default choice for tasks that do not require advanced tokenization strategies.</p>
<section id="example-usage-getting-word-bounding-box-bbox-for-a-document">
<h3>Example Usage: Getting Word Bounding Box (BBox) for a Document<a class="headerlink" href="#example-usage-getting-word-bounding-box-bbox-for-a-document" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we will walk through how to extract the bounding box (<a class="reference external" href="https://dev.konfuzio.com/sdk/sourcecode.html#bbox">BBox</a>)
for words in a Document, rather than for individual characters, using the Konfuzio SDK. This process involves the use of
the <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> from the Konfuzio SDK to tokenize the Document and identify word-level Spans, which can then
be visualized or used to extract BBox information.</p>
<section id="prerequisites">
<h4>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>You will need to have the Konfuzio SDK installed.</p></li>
<li><p>You should have access to a Project on the Konfuzio platform.</p></li>
</ul>
</section>
<section id="preview-of-result">
<h4>Preview of Result<a class="headerlink" href="#preview-of-result" title="Permalink to this headline">¶</a></h4>
<img src="https://github.com/konfuzio-ai/konfuzio-sdk/assets/2879188/5f7a8501-cd89-487d-a332-0703f3c35fc8" data-canonical-src="https://github.com/konfuzio-ai/konfuzio-sdk/assets/2879188/5f7a8501-cd89-487d-a332-0703f3c35fc8" width="200" height="400" />
</section>
<section id="steps">
<h4>Steps<a class="headerlink" href="#steps" title="Permalink to this headline">¶</a></h4>
<p>.. collapse:: Full code</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start full word_bboxes
:end-before: end full word_bboxes
:dedent: 4</p>
<br/>
1. **Import necessary modules**:
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start import
:end-before: end import
:dedent: 4</p>
<ol class="arabic" start="2">
<li><p><strong>Initialize your Project</strong>:</p>
<p>This involves creating a Project instance with the appropriate ID.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start project
:end-before: end project
:dedent: 4</p>
</li>
<li><p><strong>Retrieve a Document from your Project</strong>:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start document
:end-before: end document
:dedent: 4</p>
</li>
<li><p><strong>Create a copy of your Document without Annotations</strong>:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start copy
:end-before: end copy
:dedent: 4</p>
</li>
<li><p><strong>Tokenize the Document</strong>:</p>
<p>This process involves splitting the Document into word-level Spans using the <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code>.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start tokenize
:end-before: end tokenize
:dedent: 4</p>
</li>
<li><p><strong>Visualize all word-level Annotations</strong>:</p>
<p>After getting the bounding box for all Spans, you might want to visually check the results to make sure the bounding
boxes are correctly assigned. Here’s how you can do it:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start image
:end-before: end image
:dedent: 4</p>
<p>.. image:: /sdk/tutorials/tokenizers/word-bboxes.png</p>
<p>This will display an image of the Document with all word-level Annotations. The image may look a bit messy with all
the Labels.</p>
</li>
<li><p><strong>Get bounding box for all Spans</strong>:</p>
<p>You can retrieve bounding boxes for all word-level Spans using the following code:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_word_bboxes.py
:language: python
:start-after: start spans
:end-before: end spans
:dedent: 4</p>
<p>Each bounding box (<code class="docutils literal notranslate"><span class="pre">Bbox</span></code>) in the list corresponds to a specific word and is defined by four coordinates: x0 and y0
specify the coordinates of the bottom left corner, while x1 and y1 mark the coordinates of the top right corner,
thereby specifying the box’s position and dimensions on the Document Page.</p>
</li>
</ol>
</section>
</section>
</section>
<section id="training-a-label-specific-regex-tokenizer">
<h2>Training a Label-Specific Regex Tokenizer<a class="headerlink" href="#training-a-label-specific-regex-tokenizer" title="Permalink to this headline">¶</a></h2>
<p><strong>Pro Tip</strong>: Read our technical blog post <a class="reference external" href="https://helm-nagel.com/Automated-Regex-Generation-based-on-examples">Automated Regex</a>
to find out how we use Regex to detect outliers in our annotated data.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">konfuzio_sdk</span></code> package offers many tools for tokenization tasks. For more complex scenarios, like identifying intricate
Annotation strings, it allows for the training of a custom Regex Tokenizer. This can often be a more effective approach
than relying on a basic <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code>.</p>
<section id="example-usage">
<h3>Example Usage<a class="headerlink" href="#example-usage" title="Permalink to this headline">¶</a></h3>
<p>In this example, you will see how to find regex expressions that match with occurrences of the “Lohnart” Label in the
training data.</p>
<p>.. collapse:: Full code</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start full training
:end-before: end full training
:dedent: 4</p>
<br/>
1. **Import necessary modules**:
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start import
:end-before: end import
:dedent: 4</p>
<ol class="arabic" start="2">
<li><p><strong>Initialize your Project and retrieve the Category</strong>:</p>
<p>This involves creating a Project instance with the appropriate Project ID and retrieving the relevant Category with
the Category ID.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: # start initialize
:end-before: end initialize
:dedent: 4</p>
</li>
<li><p><strong>Initialize the ListTokenizer</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">ListTokenizer</span></code> will hold all the <code class="docutils literal notranslate"><span class="pre">RegexTokenizers</span></code> found for the Label.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start listtokenizer
:end-before: end listtokenizer
:dedent: 4</p>
</li>
<li><p><strong>Retrieve the “Lohnart” Label</strong></p>
<p>We retrieve the Label using its name. If you have its ID, you could also use the <code class="docutils literal notranslate"><span class="pre">Project.get_label_by_id</span></code> method.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start label
:end-before: end label
:dedent: 4</p>
</li>
<li><p><strong>Find Label Regexes and Create RegexTokenizers</strong></p>
<p>We then use <code class="docutils literal notranslate"><span class="pre">Label.find_regex</span></code> to algorithmically search for the best fitting regexes matching the Annotations associated with this Label.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start train
:end-before: end train
:dedent: 4</p>
</li>
<li><p><strong>Use the new Tokenizer to Create New Annotations</strong></p>
<p>Finally, we can use the Tokenizer to create new <code class="docutils literal notranslate"><span class="pre">NO_LABEL</span></code> Annotations which match with the regex patterns found.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_train_label_regex_tokenizer.py
:language: python
:start-after: start use
:end-before: end use
:dedent: 4</p>
</li>
</ol>
<p>.. _paragraph-tokenizer-tutorial:</p>
</section>
</section>
<section id="paragraph-tokenizer">
<h2>Paragraph Tokenizer<a class="headerlink" href="#paragraph-tokenizer" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">ParagraphTokenizer</span></code> class is a specialized <a class="reference external" href="https://dev.konfuzio.com/sdk/sourcecode.html#tokenizers">Tokenizer</a>
designed to create <a class="reference external" href="https://dev.konfuzio.com/sdk/sourcecode.html#annotation">Annotations</a> splitting a Document into
meaningful sections. It provides two modes of operation: <code class="docutils literal notranslate"><span class="pre">detectron</span></code> and <code class="docutils literal notranslate"><span class="pre">line_distance</span></code>.</p>
<section id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: This parameter determines the mode of operation for the
<a class="reference external" href="https://dev.konfuzio.com/sdk/sourcecode.html#tokenizers">Tokenizer</a>. It can be <code class="docutils literal notranslate"><span class="pre">detectron</span></code> or <code class="docutils literal notranslate"><span class="pre">line_distance</span></code>.
In <code class="docutils literal notranslate"><span class="pre">detectron</span></code> mode, the Tokenizer uses a fine-tuned <a class="reference external" href="https://github.com/facebookresearch/detectron2">Detectron2</a> model
to assist in Document segmentation. This mode tends to be more accurate but slower as it requires to make an API call to
the model hosted on Konfuzio servers. The <code class="docutils literal notranslate"><span class="pre">line_distance</span></code> mode, on the other hand, uses a rule-based approach that is
faster but less accurate, especially with Documents having two columns or other complex layouts. The default is
<code class="docutils literal notranslate"><span class="pre">detectron</span></code>.</p></li>
</ul>
</section>
<section id="line-distance-approach">
<h3>Line Distance Approach<a class="headerlink" href="#line-distance-approach" title="Permalink to this headline">¶</a></h3>
<p>The line_distance approach offers a straightforward, efficient way to segment Documents based on line heights, proving
especially useful for simple, single-column formats. Despite its limitations with complex layouts, its fast processing
and relatively accurate results make it a practical choice for tasks where speed is a priority and the Document
structure isn’t overly complicated.</p>
<section id="parameters-for-line-distance-approach">
<h4>Parameters for Line Distance Approach<a class="headerlink" href="#parameters-for-line-distance-approach" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">line_height_ratio</span></code>: Specifies the ratio of the median line height used as a threshold to create a new paragraph when
using the Tokenizer in <code class="docutils literal notranslate"><span class="pre">line_distance</span></code> mode. The default value is 0.8. If you notice that the Tokenizer is not creating
new paragraphs when it should, you can try lowering this value. Alternatively, if the Tokenizer is creating too many
paragraphs, you can try increasing this value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">height</span></code>: This optional parameter allows you to define a specific line height threshold for creating new paragraphs.
If set to None, the Tokenizer uses the intelligently calculated height threshold.</p></li>
</ul>
<p>For a quicker result with a relatively simpler, single-column Document, you can use the <code class="docutils literal notranslate"><span class="pre">ParagraphTokenizer</span></code> in
<code class="docutils literal notranslate"><span class="pre">line_distance</span></code> mode like this:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_paragraph_tokenizer.py
:language: python
:start-after: start init2
:end-before: end init2
:dedent: 4</p>
<p>.. image:: /_static/img/line_distance_paragraph_tokenizer.png</p>
<p>While the line_distance approach offers efficient Document segmentation for simpler formats, it can struggle with complex
layouts and diverse Document elements. The computer vision approach, using tools like Detectron2, adapts to a wide range
of layouts and provides precise segmentation even in complex scenarios. Although it may require more processing power,
the significant improvement in accuracy and comprehensiveness makes it a powerful upgrade for high-quality Document analysis.</p>
</section>
</section>
<section id="computer-vision-approach">
<h3>Computer Vision Approach<a class="headerlink" href="#computer-vision-approach" title="Permalink to this headline">¶</a></h3>
<p>With the computer vision approach, we can create Labels, identify figures, tables, lists, texts, and titles, thereby giving
us a comprehensive understanding of the Document’s structure.</p>
<p>Using the computer vision approach might require more processing power and might be slower compared to the line_distance
approach, but the significant leap in the comprehensiveness of the output makes it a powerful tool.</p>
<section id="parameters-for-computer-vision-approach">
<h4>Parameters for Computer Vision Approach<a class="headerlink" href="#parameters-for-computer-vision-approach" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">create_detectron_labels</span></code>: This boolean flag determines whether to apply Labels given by the Detectron2 model when
the Tokenizer is used in <code class="docutils literal notranslate"><span class="pre">detectron</span></code> mode. If the Labels don’t exist, they will be created. The potential Labels include
<code class="docutils literal notranslate"><span class="pre">figure</span></code>, <code class="docutils literal notranslate"><span class="pre">table</span></code>, <code class="docutils literal notranslate"><span class="pre">list</span></code>, <code class="docutils literal notranslate"><span class="pre">text</span></code> and <code class="docutils literal notranslate"><span class="pre">title</span></code>. If this option is set to False, the Tokenizer will create <code class="docutils literal notranslate"><span class="pre">NO_LABEL</span></code>
Annotations, just like with out other <a class="reference external" href="https://dev.konfuzio.com/sdk/sourcecode.html#tokenizers">Tokenizers</a>. The default
value is False.</p></li>
</ul>
<p>To tokenize a Document into paragraphs using the <code class="docutils literal notranslate"><span class="pre">ParagraphTokenizer</span></code> in <code class="docutils literal notranslate"><span class="pre">detectron</span></code> mode and the
<code class="docutils literal notranslate"><span class="pre">create_detectron_labels</span></code> option to use the Labels provided by our Detectron model, you can use the following code:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_paragraph_tokenizer.py
:language: python
:start-after: start init1
:end-before: end init1
:dedent: 4</p>
<p>The resulting Annotations will look like this:</p>
<p>.. image:: /_static/img/paragraph_tokenizer.png</p>
<p>Once you have a grasp on how to implement and utilize the ParagraphTokenizer, you open a new world of possibilities.
The segmentation of Documents doesn’t stop at the paragraph level. On the contrary, it is just the beginning. Let’s
embark on a journey to dive deeper into the Document’s structure and semantics with the SentenceTokenizer.</p>
</section>
</section>
</section>
<section id="sentence-tokenizer">
<h2>Sentence Tokenizer<a class="headerlink" href="#sentence-tokenizer" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">SentenceTokenizer</span></code> class, akin to the :ref:<code class="docutils literal notranslate"><span class="pre">ParagraphTokenizer&lt;paragraph-tokenizer-tutorial&gt;</span></code>,
is a specialized Tokenizer designed to split a Document into sentences. It also provides two modes of operation:
<code class="docutils literal notranslate"><span class="pre">detectron</span></code> and <code class="docutils literal notranslate"><span class="pre">line_distance</span></code>. And just like the <code class="docutils literal notranslate"><span class="pre">ParagraphTokenizer</span></code>, you can customize the behavior of the Tokenizer
by passing using the <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">line_height_ratio</span></code>, <code class="docutils literal notranslate"><span class="pre">height</span></code> and <code class="docutils literal notranslate"><span class="pre">create_detectron_labels</span></code> parameters. The distinguishing
feature of the <code class="docutils literal notranslate"><span class="pre">SentenceTokenizer</span></code> is that it will split the Document into sentences, not paragraphs.</p>
<section id="id1">
<h3>Example Usage<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>To use it, you can use the following code:</p>
<p>.. literalinclude:: /sdk/boilerplates/test_sentence_tokenizer.py
:language: python
:start-after: start import
:end-before: end import
:dedent: 4</p>
<p>The resulting Annotations will look like this:</p>
<p>.. image:: /_static/img/sentence_tokenizer.png</p>
</section>
</section>
<section id="how-to-choose-which-tokenizer-to-use">
<h2>How to Choose Which Tokenizer to Use?<a class="headerlink" href="#how-to-choose-which-tokenizer-to-use" title="Permalink to this headline">¶</a></h2>
<p>When it comes to Natural Language Processing (NLP), choosing the correct Tokenizer can make a significant impact on
your system’s performance and accuracy. The Konfuzio SDK offers several tokenization options, each suited to different
tasks:</p>
<ol class="arabic simple">
<li><p><strong>WhitespaceTokenizer</strong>: Perfect for basic word-level processing. This Tokenizer breaks text into chunks separated
by white spaces. It is ideal for straightforward tasks such as basic keyword extraction.</p></li>
<li><p><strong>Label-Specific Regex Tokenizer</strong>: Known as “Character” detection mode on the Konfuzio platform, this Tokenizer
offers more specialized functionality. It uses Annotations of a Label within a training set to pinpoint and tokenize
precise chunks of text. It’s especially effective for tasks like entity recognition, where accuracy is paramount. By
recognizing specific word or character patterns, it allows for more precise and nuanced data processing.</p></li>
<li><p><strong>ParagraphTokenizer</strong>: Identifies and separates larger text chunks - paragraphs. This is beneficial when your
text’s interpretation relies heavily on the context at the paragraph level.</p></li>
<li><p><strong>SentenceTokenizer</strong>: Segments text into sentences. This is useful when the meaning of your text depends on the
context provided at the sentence level.</p></li>
</ol>
<p>Choosing the right Tokenizer is a matter of understanding your NLP task, the structure of your data, and the degree of
detail your processing requires. By aligning these elements with the functionalities provided by the different
Tokenizers in the Konfuzio SDK, you can select the best tool for your task.</p>
<section id="verify-that-a-tokenizer-finds-all-labels">
<h3>Verify that a Tokenizer finds all Labels<a class="headerlink" href="#verify-that-a-tokenizer-finds-all-labels" title="Permalink to this headline">¶</a></h3>
<p>To help you choose the right Tokenizer for your task, it can be useful to try out different Tokenizers and see which
Spans are found by which Tokenizer. The <code class="docutils literal notranslate"><span class="pre">Label</span></code> class provides a method called <code class="docutils literal notranslate"><span class="pre">spans_not_found_by_tokenizer</span></code> that
can he helpful in this regard.</p>
<p>Here is an example of how to use the <code class="docutils literal notranslate"><span class="pre">Label.spans_not_found_by_tokenizer</span></code> method. This will allow you to determine if a
RegexTokenizer is suitable at finding the Spans of a Label, or what Spans might have been annotated wrong. Say, you
have a number of Annotations assigned to the <code class="docutils literal notranslate"><span class="pre">Austellungsdatum</span></code> Label and want to know which Spans would not be found
when using the Whitespace Tokenizer. You can follow this example to find all the relevant Spans.</p>
<p>.. literalinclude:: /sdk/boilerplates/test_spans_not_found_label.py
:language: python
:start-after: start spans
:end-before: end spans
:dedent: 4</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../file_splitting/index.html" class="btn btn-neutral float-left" title="File Splitting" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../information_extraction/index.html" class="btn btn-neutral float-right" title="Document Information Extraction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Helm und Nagel GmbH.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D02B3QF8Z3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-D02B3QF8Z3');
</script>
<script src="https://cmp.osano.com/16CVyMSbk3Iar1G3f/97ee6223-b0cb-4f8c-abd6-a25a0d6f6507/osano.js"></script>
<script>window._nQc="89655017";</script>
<script async src="https://serve.albacross.com/track.js"></script>
<script data-jsd-embedded data-key="42374b9a-2f7b-46ec-ae7b-b79a89aa3dd9" data-base-url="https://jsd-widget.atlassian.com" src="https://jsd-widget.atlassian.com/assets/embed.js"></script>


</body>
</html>